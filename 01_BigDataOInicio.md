# BIG DATA: O INÍCIO

## Por que Big Data?

Se você nasceu a partir de meados da década de 1990, pode se considerar um cidadão nativo da era digital. Caso tenha alguns anos adicionais, você pode se considerar um imigrante digital.

Em 2012, um dos executivos da Google, Eric Schmidt, mencionou: “Desde os primórdios da civilização até 2003, a humanidade gerou 5 Exabytes de dados. Agora, vamos produzir 5 Exabytes a cada 2 dias… e o ritmo está aumentando.” E em 2015, esse volume foi produzido em menos de 24 horas.

Poucas transformações no mundo ocorreram de forma tão rápida e expressiva, quebrando paradigmas e mudando a sociedade. Em pouco mais de uma década, impérios foram construídos sobre o domínio da tecnologia da informação. Empresas digitais como Google, Amazon e Facebook se tornaram motores de inovação e são alimentadas com um valioso combustível chamado “dados”.

A sociedade mudou, interagimos através de uma rede on-line e construímos relacionamentos virtuais com nossos familiares, amigos e pessoas que fazem parte do nosso ambiente profissional. As novas gerações (nativos da era digital) estão naturalmente conectadas e produzindo a matéria-prima para esta nova era, uma fonte inesgotável.

A economia também mudou, agora devemos acrescentar dados para compor a equação (Cesar Taurion – Big Data).
> Economia = (Capital + Trabalho + Dados)

Hoje, não fazemos negócios e não tomamos uma única decisão sem a análise criteriosa de dados. E a competência de usar os dados para direcionar a tomada de decisão é determinante para se manter competitivo nesta nova economia, seja para entender melhor a necessidade do seu cliente, para criar um novo produto, para fazer ou evitar um novo investimento e combater fraudes.

Você pode se perguntar: “Dados não fazem parte da história e evolução da humanidade?” Sim. Claro!

Temos experimentos científicos que dependem de várias pesquisas
documentadas com dados importantes, usados para correlações e estatísticas de pesquisas avançadas, fazemos isso há anos.

Na Segunda Guerra Mundial, a Inglaterra deu início à construção de uma máquina idealizada pelo matemático britânico Alan Turing, tido como o inventor do computador. O filme O Jogo da Imitação, dirigido por Morten Tyldum, conta de maneira contundente como a estratégia baseada em dados para decifrar a criptografia alemã ajudou a finalizar a guerra com anos de antecedência. A importância de analisar os dados criptografados (não estruturados) com latência abaixo de 24 horas, para se antecipar aos ataques alemães, era crucial para vencer esse desafio.

Como podemos perceber, os dados ajudaram a direcionar a evolução da humanidade de maneira expressiva, porém em um ritmo “tímido e limitado” perto da revolução que estamos iniciando.

Proponho definirmos um marco para o Big Data, vamos usar a frase de Eric Shmidt da Google e apontar para o ano de 2003:

!["Nova era Digital"](/images/novaEraDigital.png)<br/> - Nova Era Digital - Fonte: FIAP(2016).

Definitivamente, estamos usando o adjetivo “BIG” para expressar o que realmente mudou de maneira significativa, que é a dimensão de volume de dados produzidos a partir de 2003 (um marco simbólico).

Empresas como Google, Amazon, Facebook, eBay, LinkedIn, Netflix, Airbnb e Uber são exemplos de empresas nativas da era digital, e representam um novo modelo de negócios estruturado em rede. As redes permitem um crescimento orgânico e acelerado, que transpõe facilmente as fronteiras continentais. Quanto maior a escalabilidade dessas empresas, maior o potencial de criar Big Data(s). Essas empresas foram pioneiras e atingiram grande parte da população mundial. A tecnologia não estava preparada para esse nível de escalabilidade, portanto, as gigantes digitais aprenderam a primeira grande lição da era da informação.

    A inovação deve fazer parte do seu DNA. Só existe uma maneira de continuar crescendo e está diretamente ligada à criação de novas tecnologias. E a capacidade de fazer isso no prazo mais agressivo determina o sucesso dos negócios.

Empresas tradicionais de tecnologia, como Oracle, IBM, HP, Microsoft, entre outras, não almejavam criar soluções para atender startups com propósitos incompreensíveis, do ponto de vista de negócios, e talvez não imaginassem a real capacidade de transformação (inovação) dessas novas empresas. E isso é mais um fator determinante para toda a evolução da tecnologia que vamos abordar mais adiante.

As empresas que direcionam a evolução das tecnologias e arquitetura de Big Data “não” são as empresas tradicionais, e sim os gigantes da era digital. É com eles que vamos aprender sobre Big Data.

O Gartner usou a abordagem “The Nexus of Forces” em 2013 para definir os pilares para a TI nesta nova era digital, como mostra a figura abaixo:

!["The Nexus of Forces"](/images/nexusForce.png)<br/> - The Nexus of Forces - Fonte: FIAP/Gartner(2016).

A nova TI deve ser estruturada nesses quatro pilares. A partir da definição do Gartner, praticamente todas as empresas tradicionais de tecnologia passaram a adotar esse modelo para definir seus pilares de soluções no mercado de TI.

Quando analisamos uma empresa nativa digital, compreendemos que ela já tem essa estrutura bem definida, desde o startup. Isso aumenta muito o seu potencial de inovação e a coloca em vantagem para escalabilidade de Big Data.

Apesar de ser um assunto que requer grande profundidade, vamos resumir uma primeira definição para Big Data.

!["Novo mindset"](/images/newMindset.png)<br/> - Novo mindset - Fonte: FIAP(2017).

Temos uma grande história de TI e vamos entender melhor essa transição, porém, para avançarmos, é necessário um ajuste no seu mapa mental. Durante anos, acumulamos aprendizado, mitos, pequenos vícios e tudo isso  ocupa um espaço no seu “cache” (memória), bloqueando a entrada de novas maneiras de inovar e resolver problemas. Quanto mais conseguirmos trabalhar essa mudança de mindset, mais fácil será o aprendizado sobre Big Data.

Estamos na quarta revolução industrial e na terceira geração computacional, e para essa nova fase não podemos usar o mapa mental antigo.

---

## A Evolução

!["Evolução da computação"](/images/evolucaoComputacao.png)<br/> - Evolução da computação - Fonte: FIAP(2016).

* **Mainframe:** computação centralizada e monolítica de implementação relativamente simples. Poucas empresas têm acesso a computação, devido ao alto custo principalmente. Do ponto de vista de desenvolvimento, os programadores são orientados a eficiência e sabem da importância ($) de otimizar o uso de recursos.

* **Client Server:** computação distribuída, muitas soluções de hardware e softwares foram criadas, existem vários fornecedores e opções para compor uma solução, isso implica em uma arquitetura cada vez mais complexa. Na década de 80 e 90 foi fortemente trabalhado para implementar o ERP e CRM. No final da década de 90 se iniciou o “mundo Web” para transformar os negócios em modelos digitais. O desenvolvimento de software desta segunda geração foi orientado a agilidade, devido ao baixo custo de hardware foi perdida um pouco a orientação a eficiência no desenvolvimento. Com o avanço da computação cliente-servidor, a TI se tornou mais democrática e praticamente todas as empresas e pessoas possuem acesso a computação.

* **Cloud:** a computação em nuvem é o modelo de arquitetura para a terceira geração. Não se engane definir nuvem apenas como um provedor remoto de computação, mas sim um novo padrão de arquitetura para a TI, desde a engenharia de software, plataformas, implementação e principalmente infraestrutura. Os novos modelos são baseados em:
  * Flexibilidade;
  * Elasticidade;
  * Escalabilidade;
  * Economia em escala;

  A arquitetura que suporta os grandes cases de Big Data foram criadas por provedores que adotaram o modelo de arquitetura em nuvem com padrões baseados em web-scale, arquitetura open, altamente eficiente. Em relação a eficiência, imagine uma otimização de um código do Google, consumindo menos 1KB de memória por conexão ao aplicativo do Google Maps. Os ganhos em escala são fatores fundamentais para manutenção dos serviços e garantia da evolução.

Com o time-to-market cada vez mais agressivo e as necessidades de tomada de decisões real time, a TI tradicional passou a não atender mais as demandas de negócios. Devido à agilidade requerida, este é o principal ponto que impacta negativamente a segunda geração da computação. Precisamos eliminar a burocracia e os serviços operacionais, transformando a TI em uma área estratégica para as empresas, andando à frente do time-to-market, posicionando inovações que transformem os negócios e aumentem as oportunidades.

A nova arquitetura em nuvem integra os conceitos de dados operacionais e analíticos, em ambientes geograficamente distribuídos. Com capacidades elásticas on-demand, possibilitando startups como Easy Taxi, Uber, Airbnb, Instagram e Netflix, que construíram cases de referência, incluindo Big Data sobre a plataforma em Nuvem Amazon AWS.

A seguir, mais um exemplo de evolução baseado em demandas, volumes e complexidade de dados.

!["Volume e complexidade"](/images/volumeEComplexidade.svg)<br/> - Volume e complexidade - Fonte: FIAP(2016).

---

## Os 3 Vs de Big Data

O conceito mais usado para definir os fundamentos de Big Data é baseado em 3Vs. Não ignore ou questione os 3Vs neste momento, pois requer uma análise mais aprofundada para entendermos algumas questões importantes, as quais serão desenvolvidas ao longo dos próximos capítulos.

!["Os 3Vs de Big Data"](/images/3vsBigData.png)<br/> - Os 3Vs de Big Data - Fonte: FIAP(2016).

---

## Volume

Volume é um conceito relativo, o que é Big Data hoje pode não ser Big Data amanhã. Portanto, precisamos entender essa questão-chave sobre o principal fundamento de Big Data.

!["Volume relativo"](/images/volumeRelativo.png)<br/> - Volume relativo - Fonte: FIAP(2016).

Vamos definir um cenário inicial para identificar qual volume é considerado um Big Data e, nessa definição, utilizaremos o volume em petabytes (que equivale a 1.000 terabytes).

Obs.: Assim como 1 TB, alguns anos atrás, era considerado um grande volume, e havia poucos bancos de dados com essa capacidade, o petabyte será comum para a maioria das organizações, em pouco tempo.

* 10 TB é Big Data?
* 100 TB é Big Data?
* 1 PB é Big Data?

A melhor maneira de responder a essas questões é analisando as
tecnologias tradicionais, e para isso, proponho fazermos a seguinte pergunta:

    A solução tradicional (cliente-servidor), seja ela um database, uma aplicação ou um hardware, está preparada para atender este volume?
     Pense sobre: Custo ($) Capacidade (por exemplo: Armazenamento, Processamento, I/O) Arquitetura (por exemplo: Geograficamente Distribuída)
     Caso sua resposta seja não, temos aqui a principal motivação para quebrar os paradigmas tradicionais da segunda geração e buscar novas soluções que se enquadrem melhor nas suas necessidades computacionais da terceira geração.

Um database tradicional definitivamente não foi concebido para trabalhar com vários terabytes, sua estrutura foi estressada ao longo da evolução para se adaptar ao crescimento de volumes cada vez maiores. Porém, a partir da nova era da informação, a explosão de dados causou um colapso nas arquiteturas de dados tradicionais e chegamos ao seu limite técnico-financeiro, para a maioria das empresas que operam volumes de dados em petabytes.

Portanto, o V de volume para definição de Big Data está ligado a capacidades que excedem as tecnologias tradicionais.